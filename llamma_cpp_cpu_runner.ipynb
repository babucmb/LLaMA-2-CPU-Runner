{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Model in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama2():\n",
    "    print(\"Loading LLaMA 2 model...\")\n",
    "    model = Llama(model_path=MODEL_PATH, n_ctx=1024)  # Increase context length\n",
    "\n",
    "    prompt = \"Are you looking for full-time, part-time, temporary, or contract work?\"\n",
    "\n",
    "    print(f\"Generating response for: {prompt}\\n\")\n",
    "\n",
    "    output = model(\n",
    "        prompt,\n",
    "        max_tokens=200,  # Increase response length\n",
    "        temperature=0.7,  # More creative responses\n",
    "        top_k=50,         # Use top-k sampling\n",
    "        top_p=0.9,        # Use top-p sampling\n",
    "        repeat_penalty=1.1  # Avoid repeating phrases\n",
    "    )\n",
    "\n",
    "    print(\"LLaMA 2 Response:\\n\", output[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to Run LLaMA 2 on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Step 1: Install Required Packages\n",
    "def install_dependencies():\n",
    "    print(\"Installing llama-cpp-python...\")\n",
    "    os.system(\"pip install llama-cpp-python\")\n",
    "\n",
    "# Step 2: Download LLaMA 2 (CPU-Friendly Model)\n",
    "MODEL_URL = \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"\n",
    "MODEL_PATH = \"llama-2-7b.Q4_K_M.gguf\"\n",
    "\n",
    "def download_model():\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Downloading LLaMA 2 model from {MODEL_URL} ...\")\n",
    "        urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
    "        print(\"Model downloaded successfully!\")\n",
    "    else:\n",
    "        print(\"Model already exists, skipping download.\")\n",
    "\n",
    "# Step 3: Load LLaMA 2 Model and Run Inference\n",
    "def run_llama2():\n",
    "    print(\"Loading LLaMA 2 model...\")\n",
    "    model = Llama(model_path=MODEL_PATH, n_ctx=1024)  # Increase context length\n",
    "\n",
    "    prompt = \"Explain the impact of climate change on agriculture.\"\n",
    "    print(f\"Generating response for: {prompt}\\n\")\n",
    "\n",
    "    output = model(\n",
    "        prompt,\n",
    "        max_tokens=200,  # Increase response length\n",
    "        temperature=0.7,  # More creative responses\n",
    "        top_k=50,         # Use top-k sampling\n",
    "        top_p=0.9,        # Use top-p sampling\n",
    "        repeat_penalty=1.1  # Avoid repeating phrases\n",
    "    )\n",
    "\n",
    "    print(\"LLaMA 2 Response:\\n\", output[\"choices\"][0][\"text\"])\n",
    "\n",
    "\n",
    "# Execute all steps\n",
    "if __name__ == \"__main__\":\n",
    "    install_dependencies()\n",
    "    download_model()\n",
    "    run_llama2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used to check type of questions are present in the load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Load the Guanaco dataset\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Step 2: Extract sample prompts\n",
    "num_samples = 5  # Number of questions to generate\n",
    "sample_prompts = []\n",
    "\n",
    "print(\"\\nðŸ’¡ Generating sample prompts from dataset:\\n\")\n",
    "for i in range(num_samples):\n",
    "    instruction = dataset[i]['text']  # Extract instruction\n",
    "    sample_prompts.append(instruction)\n",
    "    print(f\"Prompt {i+1}: {instruction}\\n\")\n",
    "\n",
    "# Step 3: Save prompts to a file\n",
    "with open(\"sample_prompts.txt\", \"w\") as f:\n",
    "    for prompt in sample_prompts:\n",
    "        f.write(prompt + \"\\n\\n\")\n",
    "\n",
    "print(\"âœ… Sample prompts saved to 'sample_prompts.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Code: Read Dataset & Generate Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Load the Guanaco dataset\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Step 2: Extract sample prompts\n",
    "num_samples = 5  # Number of questions to generate\n",
    "sample_prompts = []\n",
    "\n",
    "print(\"\\nðŸ’¡ Generating sample prompts from dataset:\\n\")\n",
    "for i in range(num_samples):\n",
    "    instruction = dataset[i]['text']  # Extract instruction\n",
    "    sample_prompts.append(instruction)\n",
    "    print(f\"Prompt {i+1}: {instruction}\\n\")\n",
    "\n",
    "# Step 3: Save prompts to a file\n",
    "with open(\"sample_prompts.txt\", \"w\") as f:\n",
    "    for prompt in sample_prompts:\n",
    "        f.write(prompt + \"\\n\\n\")\n",
    "\n",
    "print(\"âœ… Sample prompts saved to 'sample_prompts.txt'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
